# kafka topic where the logs will be sent
fuse_kafka_topic=["logs"]
# directories fuse_kafka will listen to (launch script will try to
# create them if they don't exist)
# fuse_kafka will also ignore duplicates in absolute path in this list
fuse_kafka_directories=["/tmp/fuse-kafka-test"]
#fuse_kafka_directories=["C:/temp/fuse-kafka-test"]
# addresses of zookeepers pointing to kafka brokers of the form
# "zk1=port,zk2:port,zk3:port"
# if this is defined, fuse_kafka_brokers will be ignored
fuse_kafka_zookeepers=["127.0.0.1:2181"]
# addresses of kafka brokers of the form
# "broker1:port,broker2:port,broker3:port"
#fuse_kafka_brokers=["localhost:9092"]
# logstash-like fields which will be added to each event
# for example, this will allow to identify where the event is
# from
fuse_kafka_fields={"hostname": "test"}
# logstash-like tags to add to each events
# for example this will mark this event as a test event
fuse_kafka_tags=["test"]
# optional list of directories to stop listening to when fuse kafka
# goes to sleep
fuse_kafka_sleep=["/tmp/fuse-kafka-test"]
# by default, `fuse_kafka` will trace to stdout.
# you can specify a file to log to via
#fuse_kafka_log=["/tmp/fuse_kafka.log`"]
# you can enable debug (verbose) logging via
# if you do so, just do not write output to a watched directory,
# otherwise you'd end up with a retroaction.
#fuse_kafka_debug=["true"]
# you can optionnaly limit the bandwidth used by fuse_kafka
# if a file takes more than this bandwidth, its data won't be sent
# to kafka
# this array may be supplied two parameters:
#   - quota (the wanted quota in bytes per second)
#   - time_queue size: the maximum number of files the system tracks at any
#   given time (optional, default being 20)
#fuse_kafka_quota=[500000]
# default input plugin is inotify
# but you can select another one
#fuse_kafka_input=["read_directory_changes"]
fuse_kafka_input=["inotify"]
#fuse_kafka_input=["example"]
#fuse_kafka_input=["overlay"]
#fuse_kafka_input=["inotify_nonrecursive"]
# output plugin
fuse_kafka_output=["kafka"]
# fuse_kafka_output=["stdout"]
# encoder (default is logstash_base64)
# "date: path: text":
# fuse_kafka_encoder=["text"]
# json a la logstash:
# fuse_kafka_encoder=["logstash"]
# json a la logstash, encoding log text in base64:
# fuse_kafka_encoder=["logstash_base64"]
